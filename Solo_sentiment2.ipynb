{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tesla stock data:https://finance.yahoo.com/quote/TSLA/history?period1=1612137600&period2=1614038400&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true\n",
    "#Tesla Tweeter data : 01.02 -23.02 \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel (r'/Users/mitarbeiter/datascience_project/code_op_solo_project/Tesla.Xlsx', sheet_name='Sheet0')\n",
    "Tesla = pd.read_csv(r'/Users/mitarbeiter/datascience_project/code_op_solo_project/TSLA.csv')\n",
    "# pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install autopep8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df[['published','document_content']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published</th>\n",
       "      <th>document_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>@TSLAFanMtl That's what every shit company say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>What skills are needed to earn $100k+ as a Con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>.@CBS_Herridge .@nytpolitics .@alivitali .@Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>Can I have a Tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>Everyone who put a $100 deposit on the Tesla t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501388</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>@Saweetie @ghxttowitch purring in our Tesla’s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501389</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>#stocks helping to tank the high-flying #ARK I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501390</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>Tesla is up 185x in the 11 years since I first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501391</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>metaphysicists rather than scientists.\" ~ Niko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501392</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>@JayinShanghai @Tesla Jay - welcome</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501393 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         published                                   document_content\n",
       "0       2021-02-01  @TSLAFanMtl That's what every shit company say...\n",
       "1       2021-02-01  What skills are needed to earn $100k+ as a Con...\n",
       "2       2021-02-01  .@CBS_Herridge .@nytpolitics .@alivitali .@Mar...\n",
       "3       2021-02-01                                 Can I have a Tesla\n",
       "4       2021-02-01  Everyone who put a $100 deposit on the Tesla t...\n",
       "...            ...                                                ...\n",
       "501388  2021-02-23  @Saweetie @ghxttowitch purring in our Tesla’s ...\n",
       "501389  2021-02-23  #stocks helping to tank the high-flying #ARK I...\n",
       "501390  2021-02-23  Tesla is up 185x in the 11 years since I first...\n",
       "501391  2021-02-23  metaphysicists rather than scientists.\" ~ Niko...\n",
       "501392  2021-02-23                @JayinShanghai @Tesla Jay - welcome\n",
       "\n",
       "[501393 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['published']= pd.to_datetime(df.published)\n",
    "\n",
    "df['published'] = df['published'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df['published'].head()\n",
    "\n",
    "data=df[['published','document_content']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================\n",
    "##Tesla Stock Data\n",
    "#=========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tesla=Tesla.drop(['Twitter_mention'], axis=1)\n",
    "# Tesla.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Date'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-552d70c4ab3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#how many mentions there are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_mention_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'document_content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata_mention_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtwitter_mention_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_mention_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1550\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m         self._validate_read_indexer(\n\u001b[0m\u001b[1;32m   1553\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1644\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"loc\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Date'] not in index\""
     ]
    }
   ],
   "source": [
    "#how many mentions there are \n",
    "\n",
    "data_mention_count=data[['publish','document_content']]\n",
    "data_mention_count.rename(columns={'Date':'Date'},inplace=True)\n",
    "twitter_mention_count=data_mention_count.groupby(by=[\"Date\"]).count()\n",
    "# twitter_mention_count\n",
    "\n",
    "Tesla=pd.merge(Tesla, twitter_mention_count,  on='Date', how='inner')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>twitter_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>814.289978</td>\n",
       "      <td>842.000000</td>\n",
       "      <td>795.559998</td>\n",
       "      <td>839.809998</td>\n",
       "      <td>839.809998</td>\n",
       "      <td>25391400</td>\n",
       "      <td>18540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-02</td>\n",
       "      <td>844.679993</td>\n",
       "      <td>880.500000</td>\n",
       "      <td>842.200012</td>\n",
       "      <td>872.789978</td>\n",
       "      <td>872.789978</td>\n",
       "      <td>24346200</td>\n",
       "      <td>17737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-03</td>\n",
       "      <td>877.020020</td>\n",
       "      <td>878.080017</td>\n",
       "      <td>853.059998</td>\n",
       "      <td>854.690002</td>\n",
       "      <td>854.690002</td>\n",
       "      <td>18343500</td>\n",
       "      <td>17114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Adj Close  \\\n",
       "0  2021-02-01  814.289978  842.000000  795.559998  839.809998  839.809998   \n",
       "1  2021-02-02  844.679993  880.500000  842.200012  872.789978  872.789978   \n",
       "2  2021-02-03  877.020020  878.080017  853.059998  854.690002  854.690002   \n",
       "\n",
       "     Volume  twitter_volume  \n",
       "0  25391400           18540  \n",
       "1  24346200           17737  \n",
       "2  18343500           17114  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tesla.rename(columns={'document_content':'twitter_volume'},inplace=True)\n",
    "Tesla.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data=Tesla[['Date','Close','Volume']]\n",
    "stock_diff=stock_data.set_index('Date').diff()\n",
    "stock_diff\n",
    "Tesla=pd.merge(Tesla, stock_diff,  on='Date', how='inner')\n",
    "Tesla.head()\n",
    "Tesla.rename(columns={'Close_x':'Close','Volume_x':'Volume','Close_y':'Close_diff','Volume_y':'Volume_diff'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tesla.rename(columns={'Close_x': 'Close', 'Volume_x': 'Volume', 'Close_y':'Close_diff', 'Volume_y':'Volume_diff'}, inplace=True)\n",
    "Tesla.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***Plotting*** - #I still need to add daily average sentiment :) \n",
    "# but the foramt is the same so if it works I can just add one additional column\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bokeh.io import show, output_notebook, curdoc\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import row, column\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "line_chart = Tesla(plot_width=1000, plot_height=400, x_axis_type=\"Date\",\n",
    "                    title=\"Tesla stock data from 01.02 to 23.02\")\n",
    "\n",
    "line_chart.line(\n",
    "        x=\"Date\", y=\"Close\",\n",
    "        line_width=0.5, line_color=\"red\",\n",
    "        legend_label = \"Close\",\n",
    "        source=Tesla_for_widget\n",
    "        )\n",
    "\n",
    "line_chart.xaxis.axis_label = 'Time'\n",
    "line_chart.yaxis.axis_label = 'Price ($)'\n",
    "\n",
    "line_chart.legend.location = \"top_left\"\n",
    "\n",
    "show(line_chart)\n",
    "\n",
    "from bokeh.models import CheckboxButtonGroup\n",
    "\n",
    "checkbox_options = ['Open','High','Low','Close','Adj Close','Volume','Close_diff','Volume_diff']\n",
    "\n",
    "checkbox_grp = CheckboxButtonGroup(labels=checkbox_options, active=[0], button_type=\"success\")\n",
    "show(checkbox_grp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#testing plot widget\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import cufflinks as cf\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Tokenizers libs\n",
    "from nltk.tokenize import sent_tokenize, TweetTokenizer\n",
    "from nltk import word_tokenize, ne_chunk, ngrams, pos_tag\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Global Parameters\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install Libraries\n",
    "# !pip install textblob\n",
    "# !pip install tweepy\n",
    "# !pip install pycountry\n",
    "# !pip install wordcloud\n",
    "# !pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet_text(tweet):\n",
    "\n",
    "    tweet= tweet.lower()\n",
    "    # Remove urls\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\\?+\", '', str(tweet), flags=re.MULTILINE)\n",
    "    # Remove user @ references and '#' from tweet\n",
    "    tweet = re.sub(r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s\\.\\,\\!\\?\\*]' ,'', str(tweet))\n",
    "    # Remove punctuations\n",
    "#     tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords\n",
    "    tknz = TweetTokenizer()\n",
    "    tweet_tokens = tknz.tokenize(str(tweet))\n",
    "    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
    " \n",
    "    #ps = PorterStemmer()\n",
    "    #stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in filtered_words]\n",
    " \n",
    "    #count most frequent words\n",
    "#     import nltk\n",
    "# #     allWords = tknz.tokenize(str(data.document_content))\n",
    "#     all_words=str(lemma_words)\n",
    "#     allWordDist = nltk.FreqDist(w.lower() for w in all_words)\n",
    "#     stopwords = nltk.corpus.stopwords.words('english')\n",
    "#     allWordExceptStopDist = nltk.FreqDist(w.lower() for w in all_words if w not in stopwords)  \n",
    "#     mostCommon= allWordDist.most_common(30)\n",
    "#     print(mostCommon)\n",
    "    \n",
    "#     return \" \".(lemma_words)\n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['token']=data['document_content'].apply(preprocess_tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# from bokeh.io import show, output_notebook, curdoc\n",
    "# from bokeh.plotting import figure\n",
    "# from bokeh.layouts import row, column\n",
    "# from bokeh.resources import INLINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo pip install ipywidgets\n",
    "# jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipywidgets import interact,interactive,fixed,interact_manual\n",
    "# import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "# from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "# drop_down=widgets.Dropdown(\n",
    "#     options=data.columns.values,\n",
    "#     value='published',\n",
    "#     description='Column:',\n",
    "#     disabled=False,\n",
    "# )\n",
    "\n",
    "# def update_barchart(columns):\n",
    "#     return data.groupby(columns).size().plot(kind='bar',stacked=True)\n",
    "\n",
    "# interactive(update_barchart, columns=drop_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mostCommonWord(self, paragraph: str, banned: List[str]) -> str:\n",
    "#         #1). replace the punctuations with spaces,\n",
    "#         #      and put all letters in lower case\n",
    "#         normalized_str = ''.join([c.lower() if c.isalnum() else ' ' for c in paragraph])\n",
    "\n",
    "#         #2). split the string into words\n",
    "#         words = normalized_str.split()\n",
    "\n",
    "#         word_count = defaultdict(int)\n",
    "#         banned_words = set(banned)\n",
    "\n",
    "#         #3). count the appearance of each word, excluding the banned words\n",
    "#         for word in words:\n",
    "#             if word not in banned_words:\n",
    "#                 word_count[word] += 1\n",
    "\n",
    "#         #4). return the word with the highest frequency\n",
    "#         return max(word_count.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**************************** plotting ***********************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y nodejs\n",
    "# !pip install --upgrade jupyterlab\n",
    "# !pip labextension install @jupyter-widgets/jupyterlab-manager\n",
    "# !pip labextension install jupyter-matplotlib\n",
    "# !pip nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['token'].astype(str)\n",
    "#daily average compound \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "# from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "df['compound'] = [analyser.polarity_scores(x)['compound'] for x in data['document_content']]\n",
    "df['neg'] = [analyser.polarity_scores(x)['neg'] for x in data['document_content']]\n",
    "df['neu'] = [analyser.polarity_scores(x)['neu'] for x in data['document_content']]\n",
    "df['pos'] = [analyser.polarity_scores(x)['pos'] for x in data['document_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=df[['published','document_content','authors_name','sentiment_display','compound']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##let's plot this sentiment##\n",
    "doc.head()\n",
    "doc.rename(columns={'published':'Date'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_avg=doc.groupby(['Date'])['compound'].mean()\n",
    "sentiment_avg=pd.DataFrame(sentiment_avg)\n",
    "# Tesla=Tesla.drop(columns=['sentiment_avg', 'sentiment_avg_'])\n",
    "\n",
    "Tesla_sentiment=pd.merge(Tesla,sentiment_avg,how='inner',on='Date')\n",
    "Tesla_sentiment\n",
    "Tesla_sentiment.rename(columns={'compound':\"daily_sentiment_avg\"},inplace=True)\n",
    "Tesla_sentiment\n",
    "\n",
    "Tesla=Tesla_sentiment['daily_sentiment_avg']*3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-50-91156642277c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-50-91156642277c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ***plotting***\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "***plotting***\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bokeh.io import show, output_notebook, curdoc\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import row, column\n",
    "from bokeh.resources import INLINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = df['sentiment_display'].value_counts().to_dict()\n",
    "# print(counts)\n",
    "#( 1: 203516, -1: 38449}\n",
    "# df_daily_avg = df['compound'].resample('d').mean()\n",
    "twitter_mention_count=data_mention_count.groupby(by=[\"Date\"]).count()\n",
    "# twitter_mention_count\n",
    "Tesla_sentiment\n",
    "\n",
    "# pd.concat(twitter_mention_count,Tesla_sentiment,ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_mention_count['Date']=['2021-02-01','2021-02-02','2021-02-03','2021-02-04','2021-02-05','2021-02-06','2021-02-07','2021-02-08','2021-02-09','2021-02-10','2021-02-11','2021-02-12','2021-02-13','2021-02-14','2021-02-15','2021-02-16','2021-02-17','2021-02-18','2021-02-19','2021-02-20','2021-02-21','2021-02-22','2021-02-23']\n",
    "# twitter_mention_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib pyplot provides plotting API\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "# For output plots inline in notebook:\n",
    "%matplotlib inline\n",
    "# For interactive plot controls on MatplotLib output:\n",
    "# %matplotlib notebook\n",
    "# Set the default figure size for all inline plots\n",
    "# (note: needs to be AFTER the %matplotlib magic)\n",
    "plt.rcParams['figure.figsize'] = [8, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show the top 20 users in a bar plot with Matplotlib.\n",
    "# top_n = 23\n",
    "# # Create the bars on the plot\n",
    "# plt.bar(x=range(top_n), # start off with the xticks as numbers 0:19\n",
    "#         height=twitter_mention_count[0:top_n]['document_content'])\n",
    "# # Change the xticks to the correct user ids\n",
    "# plt.xticks(range(top_n), twitter_mention_count[0:top_n]['Date'], \n",
    "#            rotation=60)\n",
    "# # Set up the x, y labels, titles, and linestyles etc.\n",
    "# plt.ylabel(\"Twitter Mentions\")\n",
    "# plt.xlabel(\"Day\")\n",
    "# plt.title(\"Daily Twitter Mention\")\n",
    "# plt.gca().yaxis.grid(linestyle=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a bar plot with seaborn\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.barplot(\n",
    "    x='Date', \n",
    "    y='document_content', \n",
    "    color='salmon', \n",
    "    data=twitter_mention_count[0:23]\n",
    ")\n",
    "# Again, Matplotlib style formatting commands are used\n",
    "# to customise the output details.\n",
    "plt.xticks(rotation=60)\n",
    "plt.ylabel(\"Number of mentions\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.title(\"Number of Mentions\")\n",
    "plt.gca().yaxis.grid(linestyle=':')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####how can i get the daily average sentimetn analysis???!?!?!####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c plotly plotly --yes\n",
    "# !conda install -c conda-forge cufflinks-py --yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly --upgrade\n",
    "# !pip install cufflinks --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "# Going offline means you plot only locally, and dont need a plotly username / password\n",
    "cf.go_offline()\n",
    "# Create an interactive bar chart:\n",
    "twitter_mention_count[0:20].iplot(\n",
    "    x='Date',\n",
    "    y='document_content',\n",
    "    kind='bar'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall sentiment \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# if using a Jupyter notebook, include:\n",
    "%matplotlib inline\n",
    "\n",
    "# Pie chart\n",
    "labels = ['positive','negative']\n",
    "sizes = [203516,38449]\n",
    "# only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
    "explode = (0, 0.1,)  \n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle\n",
    "ax1.axis('equal')  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**** wordcloud**** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is the most frequent words?\n",
    "\n",
    "data['document_content']=data['document_content'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\n",
    "data['document_content']=data['document_content'].str.replace('[^\\w\\s]',' ')\n",
    "data['document_content']=data['document_content'].str.replace('\\d',' ')\n",
    "\n",
    "#stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "sw=stopwords.words('english')\n",
    "data['document_content']=data['document_content'].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n",
    "\n",
    "#lemma\n",
    "from textblob import Word\n",
    "nltk.download('wordnet')\n",
    "data['document_content']=data['document_content'].apply(lambda x:\" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "#punctuation\n",
    "data['document_content']=data['document_content'].str.replace(\"rt\",\" \")\n",
    "\n",
    "data['document_content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "=======wordcloud is not showing=========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud is not showing \n",
    "\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text=\"\".join(i for i in data.document_content)\n",
    "\n",
    "wc=WordCloud(background_color=\"black\").generate(str(data.document_content))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##named entiity recognition\n",
    "# !pip install - U spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install scispacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df\n",
    "tex=df.document_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=en_core_sci_sm.load()\n",
    "doc=nlp(text)\n",
    "displacy_image=displacy,render(doc,jupyeter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Sentiment Distribution pie chart\n",
    "\n",
    "# from textblob import TextBlob\n",
    "# def sentiment_a(df):\n",
    "#     text=data['document_content']\n",
    "#     for i in range(0,len(text)):\n",
    "#         textB=TextBlob(text[i])\n",
    "#         sentiment_a=textB.sentiment.polarity\n",
    "#         df.at[i,\"sentiment_twitter\"]=sentiment_a\n",
    "        \n",
    "#         if sentiment_a < 0.00:\n",
    "#             sentiment= \"negative\"\n",
    "#             df.at[i,\"sentiment\"]=sentiment\n",
    "            \n",
    "#         elif sentiment_a> 0.45:\n",
    "#             sentiemnt= \"positive\"\n",
    "#             df.at[i,\"sentiment\"]=sentiment\n",
    "            \n",
    "#         else:\n",
    "#             sentiment=\"netural\"\n",
    "#             df.at[i,\"sentiment\"]=sentiment\n",
    "#     return sentiment_a\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_a(data['document_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy -> to find the most mentioned words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(“en_core_web_lg”)\n",
    "# doc = nlp(“At midnight the doorbell rang, startling him fearfully.”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========let's plot the sentiment================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show(scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df=data[['published','compound']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.set_index('Date').groupby(pd.Grouper(freq='d')).mean().dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #I want to simplify it \n",
    "# def sentiment_analyzer_scores(text):\n",
    "#     score = analyser.polarity_scores(text)\n",
    "#     lb = score['compound']\n",
    "#     if lb >= 0.5:\n",
    "#         return 1\n",
    "#     elif (lb > -0.5) and (lb < 0.5):\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_analyzer_scores(data['document_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(data.token).value_counts().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def preprocess_tweet_text(tweet):\n",
    "\n",
    "# # #     tweet= tweet.lower()\n",
    "# #     # Remove urls\n",
    "# #     tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\\?+\", '', str(tweet), flags=re.MULTILINE)\n",
    "# #     # Remove user @ references and '#' from tweet\n",
    "# #     tweet = re.sub(r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s\\.\\,\\!\\?\\*]' ,'', str(tweet))\n",
    "# #     # Remove punctuations\n",
    "# # #     tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "# #     # Remove stopwords\n",
    "# #     tknz = TweetTokenizer()\n",
    "# #     tweet_tokens = tknz.tokenize(str(tweet))\n",
    "# #     filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
    " \n",
    "# #     #ps = PorterStemmer()\n",
    "# #     #stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "# #     lemmatizer = WordNetLemmatizer()\n",
    "# #     lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in filtered_words]\n",
    " \n",
    "# def most_frequent(tweet):\n",
    "#     tknz = TweetTokenizer()\n",
    "#     import nltk\n",
    "#     tweet = re.sub(r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s\\.\\,\\!\\?\\*\\:\\]' ,'', str(tweet))\n",
    "#     tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\\?+\", '', str(tweet), flags=re.MULTILINE)\n",
    "#     allWords = tknz.tokenize(str(tweet))\n",
    "#     all_words=str(allWords)\n",
    "#     allWordDist = nltk.FreqDist(w.lower() for w in all_words)\n",
    "#     stopwords = nltk.corpus.stopwords.words('english')\n",
    "#     allWordExceptStopDist = nltk.FreqDist(w.lower() for w in all_words if w not in stopwords)  \n",
    "#     mostCommon= allWordDist.most_common(50)\n",
    "#     print(mostCommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #most frequent words\n",
    "# frequent=most_frequent(data.document_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement WorldCloud (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for WorldCloud\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'word_cloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-3efbf66ec456>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install WorldCloud'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mword_cloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_cloud_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'word_cloud'"
     ]
    }
   ],
   "source": [
    "# !pip install WorldCloud\n",
    "# from word_cloud.word_cloud_generator import WordCloud\n",
    "# from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. most often mentioned words\n",
    "#2. word cloud - positive\n",
    "#3. word cloud- negative\n",
    "#4. word cloud- netural "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "# print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# # Find named entities, phrases and concepts\n",
    "# for entity in doc.ents:\n",
    "#     print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = preprocess(ex)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
